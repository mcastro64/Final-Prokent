---
title: "D607-Project3-scraper"
author: "Ariba Mandavia, Jose Fuentes, Marco Castro, Steven Gonzalez"
date: "2024-10-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(httr)
library(purrr)
library(stringr)
library(rvest)
```

## Overview

This script scrapes the [Zillow] ("https://www.zillow.com/").

```{r init-scraper}

# Read first search 

#https://www.zillow.com/buffalo-ny-14201/rentals/?searchQueryState=%7B%22pagination%22%3A%7B%7D%2C%22isMapVisible%22%3Atrue%2C%22mapBounds%22%3A%7B%22north%22%3A42.908662514964306%2C%22south%22%3A42.8828821369165%2C%22east%22%3A-78.86731258184815%2C%22west%22%3A-78.90542140753175%7D%2C%22regionSelection%22%3A%5B%7B%22regionId%22%3A63458%2C%22regionType%22%3A7%7D%5D%2C%22filterState%22%3A%7B%22fr%22%3A%7B%22value%22%3Atrue%7D%2C%22fsba%22%3A%7B%22value%22%3Afalse%7D%2C%22fsbo%22%3A%7B%22value%22%3Afalse%7D%2C%22nc%22%3A%7B%22value%22%3Afalse%7D%2C%22cmsn%22%3A%7B%22value%22%3Afalse%7D%2C%22auc%22%3A%7B%22value%22%3Afalse%7D%2C%22fore%22%3A%7B%22value%22%3Afalse%7D%2C%22beds%22%3A%7B%22min%22%3A1%2C%22max%22%3A1%7D%7D%2C%22isListVisible%22%3Atrue%2C%22mapZoom%22%3A15%2C%22usersSearchTerm%22%3A%22Buffalo%20NY%2014201%22%7D

user_agent <- "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36 Edge/18.17763"
domain <- "https://www.zillow.com"
city <- "buffalo"
state <- "ny" 
zip <- "14201"
beds <- 2
path <-paste(domain, "/", zip, "/rentals/", sep="") #, city, "-", state, "-"


set_config(add_headers(`User-Agent` = user_agent))

throttled_GET <- slowly(
  ~read_html(.),
  rate = rate_delay(2)
)

realestate_data <- throttled_GET(path)
# grab html on this page
# realestate_data 

realestate_data

# get the number of jobs per page
listings <- realestate_data %>% 
  html_element(xpath = '//div[@class="result-list-container"]' ) %>%  # placardContainer
  html_text() 

listings

```

This section crawls through each search result page and grabs the unique job listing id. This will be used later to crawl the individual job listing page.

```{r crawl-search-result-pages}

# init the page counter
current_page <- 1

# init the page_ids data frame
page_ids <- data.frame()

# loop though all pages
while (current_page < last_page) {

    # This will concatenate the url depending on the page
    page_url = str_c('https://www.reed.co.uk/jobs/full-time-data-scientist-jobs-in-london?proximity=1000&pageno=', current_page, sep="")
    page_content = read_html(page_url)
    
    # This will get the url
    page_id <- page_content %>% 
      html_elements(xpath = '//article[@class="card job-card_jobCard__MkcJD"]') %>% 
      html_nodes('header') %>% 
      html_element('a') %>% 
      html_attr('data-id')
    
    # remove all blank elements
    page_id <- page_id[!is.na(page_id)]
    
    # This appends the data together
    page_ids <- append(page_ids, page_id)
  
    # This tells us to go to the next page
    current_page <- current_page + 1
}


```

This section crawls each individual job listing page, grabs the content and exports as a parquet file.

```{r crawl-job-listing-pages}

# init all_jobs dataframe
col_names <- c("id", "job_title", "date_posted", "job_desc", "job_location", "job_salary", "job_type", "company", "company_url", "job_skills")
all_jobs <- data.frame(matrix(ncol=10,nrow=0, dimnames=list(NULL,  col_names)))

for (i in unique(page_ids)) {

  job_listing_page = read_html(str_c('https://www.reed.co.uk/jobs/data-scientist/',i, sep=""))
  
  # a helper funtion to grab the html text from a node
  get_listing_el <- function(el) {

    el_text <- job_listing_page |>
    html_elements(xpath = el) |>
    html_text() 
    
    # if no node found or empty, fill value to blank char
    if (identical(el_text, character(0))) {
      el_text <- ""
    } else {
      el_text <- str_trim(el_text, side = "both")
    }
    
    el_text
  }
  
   # get the posted date
  date_posted <- job_listing_page |>
    html_element('[itemprop="datePosted"]') |>
    html_attr("content")
  
  # get the job title
  job_title <- job_listing_page |>
    html_element("h1") |>
    html_text() 

  # get the job description
  job_desc <- get_listing_el('//div[@class="description"]') 
    
  # get sthe salery
  job_salary <- get_listing_el('//span[@data-qa="salaryLbl"]') 

  # get the location
  job_location <- get_listing_el('//span[@data-qa="localityLbl"]') 

  # get the job_type
  job_type <- get_listing_el('//span[@data-qa="jobTypeLbl"]') |> 
    str_replace_all("\\s([\\s])+","") 

  # get the company name
  company <- job_listing_page |>
    html_element('[itemprop="hiringOrganization"]') |>
    html_element('span') |>
    html_text()
  
  # company url -
  company_url <- job_listing_page |>
    html_element('[itemprop="hiringOrganization"]') |>
    html_element('[itemprop="url"]') |>
    html_attr("content")
  
   
  # get the skillls
  job_skills <- get_listing_el('//ul[@class="list-unstyled skills-list"]') |> 
    str_replace_all("\\s([\\s])+",",")
  
  # create a temporary row vector
  temp <- c(i, job_title, date_posted, job_desc, job_location, job_salary, job_type, company, company_url, job_skills)
  
  # append row to all_jobs dataframe 
  all_jobs <- rbind(temp, all_jobs) 
}
```

Once this job is complete, export the dataframe as a Parquet and CSV file. 

``` {r export-df}

# reset dataframe column names
colnames(all_jobs) <- col_names

# export files to parquet and csv
write_parquet(all_jobs, "datasets/uk-jobs.parquet")
write.csv(all_jobs, "datasets/uk-jobs.csv", row.names=FALSE)
```

## Next Steps
As next steps, we could follow the[ERD](https://miro.com/app/board/uXjVLPb_3f0=/) from our Indeed database to normalize this dataset. This would allow us to merge the two datasets to aggregrate the information accross the two datasets and/or compare differences between US and UK-based top Data Science skills sought by companies this year. Ideally though, this dataset would match the same time-period of our original "control" dataset. Additionally, this dataset needs heavy cleaning and tidying.
